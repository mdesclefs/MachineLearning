{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homeworks\n",
    "\n",
    "For the course of Statistical Foundations of Machine Learning, two homeworks had to be done with a notebook description that will be useful for the presentation during the exam.\n",
    "The selected homeworks are homework number 1 (over the Perceptron Learning Algorithm) and homework number 4 (over overfitting and Regularization with Weight Decay).\n",
    "\n",
    "The goals of those homeworks are to implement both techniques and to answer questions over them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 : The Perceptron Learning Algorithm\n",
    "\n",
    "Perceptron is a supervised learning algorithm of linear classifiers. This kind of algorithm uses a training set, which is made of output depending on inputs examples, and try to produce an inferred function by analysing the training set. In this work, the perceptron only has one layer and one output. It means that all the inputs are directly connected to the output which is 1 or -1. This algorithm also uses the notion of weight.\n",
    "\n",
    "## The goal\n",
    "\n",
    "After creating a random target function _f_ and a data set _D_. We have to implement the algorithm to see how it works. Inputs are tuple of two values in _X_ = [-1,1] x [-1,1]. Since we implement a classifier, the output variable takes class labels and not a continious value which is the case for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### The Training Set\n",
    "\n",
    "So the first step is to create the Training Set ([_**x**(n),y(n)_)] which is made of point.\n",
    "A target function is generated based on two randoms points in _X_ (the attributes of the data, d=2). For each point in the training set, we have 3 values : the input made of two values and the output value. Then _**x**(n)_ points are generated and according to the point values, the corresponding _y_ is calculated. To do so, we only have to know if the point is in the left (1) or the right (-1) part of the target function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def constructTargetFunction(self, N):\n",
    "    # Construct random target function f\n",
    "    rPA = self.getRandomPoint()\n",
    "    rPB = self.getRandomPoint()\n",
    "    self.target = [rPA, rPB]\n",
    "    \n",
    "def constructTrainingSet(self, N):\n",
    "    self.dataset = []\n",
    "    \n",
    "    # Construct training set : [ [point(x,y), [+, -]], ...]\n",
    "    for i in range(N):\n",
    "        x = [self.getRandom() for i in range(self.d)]\n",
    "        self.dataset.append([x, self.getY(x)])\n",
    "\n",
    "def getY(self, point):\n",
    "        if ( ((self.target[1][0]-self.target[0][0])*(point[1]-self.target[0][1])) - ((self.target[1][1]-self.target[0][1])*(point[0]-self.target[0][0])) > 0): \n",
    "            return 1\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Perception Learning Algorithm\n",
    "\n",
    "As long as the training set is built up, the training function has to be trained. Indeed the training function which mainly depends on the weight vector tries to converge to the target function by adjusting this weight vector. Each attribute of our data has a specific weight. This training function is called **_h_(x)**.\n",
    "\n",
    "**_h_**(x) = sign((w^(t))\\*x).\n",
    "\n",
    "At first, the weight vector is initialised to zero. So, the training function and the target function are badly different. This technique consists of picking at each step a _misclassified_ point and then updates the weight vector depending on the previous picked point. The way to update the weight vector is :\n",
    "\n",
    "w <- w + y(n)**x**(n) where **x**(n) is the _misclassified_ point.\n",
    "\n",
    "A point is considered as _misclassified_ as long as its value through the training function (h(x(i))) is different from the real one (y(i)).\n",
    "\n",
    "The following code executes a typical run of the **Perceptron Learning Algorithm (_PLA_)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def PLA(self, maxIteration):\n",
    "    \"\"\"\" Executes a run of PLA \"\"\"\n",
    "    self.initWeightVector()\n",
    "    iterationNbr = 0\n",
    "    while iterationNbr < maxIteration : # The classification is bounded to 5000 turns\n",
    "        # Construct the misclassified point vector\n",
    "        self.misclassifiedVector = self.constructMisclassifiedVector()\n",
    "\n",
    "        # As long as misclassified point exists\n",
    "        if (len(self.misclassifiedVector) == 0):\n",
    "            break\n",
    "        mcX = self.pickMisclassified()\n",
    "\n",
    "        #Update the weight vector\n",
    "        self.updateW(mcX)\n",
    "        iterationNbr+=1\n",
    "\n",
    "    return iterationNbr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a great idea of the efficiency of **PLA**. We will run the algorithm over 1000 turns and by means of means, we will get average values for e.g. number of iteration or misclassified point probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(self, N, turn):\n",
    "    \"\"\" Run (turn) turns of PLA with N points\"\"\"\n",
    "    probTot = 0\n",
    "    convergeTot = 0\n",
    "\n",
    "    for i in range(turn):\n",
    "        self.constructTargetFunction()\n",
    "        self.constructTrainingSet(N)\n",
    "\n",
    "        convergeTot+=self.PLA(2000)\n",
    "        probTot+=self.computeMissedProbability(turn)\n",
    "\n",
    "    avgConverge = convergeTot/turn\n",
    "    missedProb = probTot/turn\n",
    "    print(\"Average iteration convergence: \"+str((avgConverge)))\n",
    "    print(\"Missed probability: \"+str((missedProb)))\n",
    "\n",
    "    self.normalizeConfusionMatrix(turn*turn)\n",
    "\n",
    "    return avgConverge, missedProb, self.confusionMatrix\n",
    "    \n",
    "def computeMissedProbability(self, turn):\n",
    "    \"\"\" Computes the probability that a point is misclassified \"\"\"\n",
    "    probTmp = 0\n",
    "    for j in range(turn):\n",
    "        p = self.getRandomPoint()\n",
    "        y = self.getY(p)\n",
    "        estimatedY = self.getH(p)\n",
    "\n",
    "        if(y != estimatedY):\n",
    "            probTmp+=1\n",
    "\n",
    "        self.setConfusionClass(y,estimatedY)\n",
    "\n",
    "    return probTmp/turn\n",
    "\n",
    "def setConfusionClass(self, y, h):\n",
    "    \"\"\" Adds a confusion in the matrix \"\"\"\n",
    "    if (y==h): # Good estimation => True\n",
    "        if(y==1): # Should have been 1 => true positive\n",
    "            self.confusionMatrix['tp']+=1\n",
    "        else:\n",
    "            self.confusionMatrix['tn']+=1\n",
    "    else: # Bad estimation => False\n",
    "        if(y==1): # Should have been 1 => false negative\n",
    "            self.confusionMatrix['fn']+=1\n",
    "        else:\n",
    "            self.confusionMatrix['fp']+=1\n",
    "\n",
    "def normalizeConfusionMatrix(self, turn):\n",
    "    for key in self.confusionMatrix:\n",
    "        self.confusionMatrix[key]/=turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average iteration convergence: 9.811\n",
      "Missed probability: 0.10759400000000001\n"
     ]
    }
   ],
   "source": [
    "from perceptron import Perceptron\n",
    "\n",
    "perceptron = Perceptron(d=2)\n",
    "#print(\"N = 10\")\n",
    "perceptron.run(10, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.** For N = 10, how many iterations does it take on average for the PLA to converge ?\n",
    "\n",
    ">**[b] 15**\n",
    "\n",
    "**8.** Which probability is the closest to missed probability ?\n",
    "\n",
    ">**[c] 0.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average iteration convergence: 101.915\n",
      "Missed probability: 0.013002999999999959\n"
     ]
    }
   ],
   "source": [
    "#print(\"N = 100\")\n",
    "perceptron.run(100, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.** For N = 10, how many iterations does it take on average for the PLA to converge ?\n",
    "\n",
    ">**[b] 100**\n",
    "\n",
    "**10.** Which probability is the closest to missed probability ?\n",
    "\n",
    ">**[b] 0.01**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution \n",
    "\n",
    "The two factors that we just studied are the number of iteration and the misclassification of point probability over the length of the training set **N**. The two questions was about using _N=10_ and _N=100_. So it could interresting to see how the length affects those factors from **N** in a range from 0 to 100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img width=\"49%\" style=\"border: 1px solid black;margin:0\" src=\"images/nEvolutionMisclassifiedprobabilityOk.png\" align=\"left\">\n",
    "    <img width=\"49%\" style=\"border: 1px solid black;margin:0\" src=\"images/nEvolutionNumberofiterationOk.png\" align=\"right\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see that increasing the training set length tend to slow down PLA's time execution. In opposite, increasing the training set length make the classifier more efficient. Indeed we can easily see on the first figure that the misclassified probability tend to 0 for **N** = 0. As long as the two factors go in different way. It could be interesting to evaluate the most effecient length **N** for the training set. Since the number of iteration is linear, a quick analysis is to compares slopes at both points of **N** and pick the first one when they are going to converge (especially the missed classification probability because of the linearity of the other one). Then, we can see on the following figure that an efficient **N** could be _20_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img width=\"50%\" style=\"border: 1px solid black;padding:0 10px 0 0\" src=\"images/nEvolution_slope.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A **confusion matrix** is a tool to evaluate the quality of a classifier.<br />\n",
    "Each row represent the number of occurences predicted by the classifier and each column represent the number of real occurences. It usually use the number of occurences but since we run several turns, we will directly use the probability over a data set of 100 points.<br />\n",
    "\n",
    "Based on this matrix, we can use the kappa score to quantify the quality of the classified.\n",
    "It exists several interpretation of the Cohenâ€™s kappa. We will use the one illustrated in _[Marry L. McHugh. Interrater reliability: the kappa statistic. Biochemia Medica 2012;22(3):276-82]_\n",
    "\n",
    "#### N = 10\n",
    "\n",
    "Confusion Matrix = ['fp': 0.054089, 'fn': 0.059407, 'tn': 0.440985, 'tp': 0.445519]\n",
    "\n",
    "| Real/Predicted   | 1  | -1  |\n",
    "|---|---|---|\n",
    "| 1  | 45%  | 6%  | \n",
    "| -1 | 5%  | 44%  |\n",
    "\n",
    "+ **Ground truth:** 50, 50<br />\n",
    "+ **Machine learning:** 51, 49<br />\n",
    "+ **Total:** (100)<br />\n",
    "<br />\n",
    "+ **Observed Accuracy:** ((45+44) / 100) = 0.89<br />\n",
    "+ **Expected Accuracy:** ((50\\*51/100) + (50\\*49/100)) / 100 = 0.50<br />\n",
    "+ **Kappa:** (0.89 - 0.50) / (1 - 0.50) = 0.78<br />\n",
    "<br />\n",
    "+ **Result:** between **Moderate** (.60-.79) and **Strong** (.80-.90)<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N = 100\n",
    "\n",
    "Confusion Matrix = ['tn': 0.492615, 'fp': 0.007225, 'tp': 0.493233, 'fn': 0.006927]\n",
    "\n",
    "| Real/Predicted   | 1  | -1  |\n",
    "|---|---|---|\n",
    "| 1  | 49%  | 1%  | \n",
    "| -1 | 1%  | 49%  |\n",
    "\n",
    "+ **Ground truth:** 50, 50<br />\n",
    "+ **Machine learning:** 50, 40<br />\n",
    "+ **Total:** (100)<br />\n",
    "<br />\n",
    "+ **Observed Accuracy:** ((49+49) / 100) = 0.98<br />\n",
    "+ **Expected Accuracy:** ((50\\*50/100) + (50\\*40/100)) / 100 = 0.45<br />\n",
    "+ **Kappa:** (0.98 - 0.45) / (1 - 0.45) = 0.96<br />\n",
    "<br />\n",
    "+ **Result:** **Almost Perfect** (above .96)<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Homework 6 : Overfitting and Regularization with Weight Decay.\n",
    "\n",
    "One of the most common tasks in machine learning is trying to find the target function of a training data set. Based on this target function, we can make reliable predictions on untrained data. The main goal is thus to **fit** the target function. Overfitting occurs when the model we are using to fit the target function is too much complex.\n",
    "\n",
    "\n",
    "There are **two cures** for overfitting :\n",
    "\n",
    "    1. Regularization\n",
    "    2. Validation\n",
    "\n",
    "In this work, we only focus on **Regularization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
